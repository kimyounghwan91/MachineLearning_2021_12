{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 형태소 분석기 - SOYNLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.493'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soynlp\n",
    "soynlp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/2016-10-20.txt', <http.client.HTTPMessage at 0x14cdbb1d4c0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\", filename=\"data/2016-10-20.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SOYNLP에서 사용될 단어 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30091"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터를 다수의 문서로 분리\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "corpus = DoublespaceLineCorpus(\"data/2016-10-20.txt\")\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.752 Gb\n",
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 361598\n",
      "all accessor variety was computed # words = 361598\n"
     ]
    }
   ],
   "source": [
    "# 전체 말뭉치에서 단어 점수표를 계산 (학습)\n",
    "from soynlp.word import WordExtractor\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(corpus)\n",
    "word_score_table = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('국제사회', '와'), ('우리', '의'), ('노력', '들로'), ('범죄', '를'), ('척결', '하자')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "scores = {word:score.cohesion_forward for word, score in word_score_table.items()}\n",
    "l_tokenizer = LTokenizer(scores=scores)\n",
    "l_tokenizer.tokenize(\"국제사회와 우리의 노력들로 범죄를 척결하자\", flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/scores.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(scores, 'data/scores.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 403896 from 30091 sents. mem=0.822 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4434442, mem=1.454 Gb\n",
      "[Noun Extractor] batch prediction was completed for 119705 words\n",
      "[Noun Extractor] checked compounds. discovered 70639 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 109312 -> 92205\n",
      "[Noun Extractor] postprocessing ignore_features : 92205 -> 91999\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91999 -> 90643\n",
      "[Noun Extractor] 90643 nouns (70639 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.575 Gb                    \n",
      "[Noun Extractor] 76.63 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "from soynlp.noun import LRNounExtractor_v2\n",
    "noun_extractor = LRNounExtractor_v2(verbose=True)\n",
    "nouns = noun_extractor.train_extract(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/noun_scores.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_scores = {noun:score[0] for noun, score in nouns.items() if len(noun) > 1}\n",
    "joblib.dump(noun_scores, 'data/noun_scores.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
