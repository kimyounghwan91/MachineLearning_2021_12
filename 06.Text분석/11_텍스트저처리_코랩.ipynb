{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11.텍스트저처리-코랩",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. 토근화(Tokenization)"
      ],
      "metadata": {
        "id": "PCfpSJiGnyac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfaHWLQrmCE-",
        "outputId": "2bca40f5-3ea7-4f21-b8ba-daa71a0ecd2a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) 단어 토큰화"
      ],
      "metadata": {
        "id": "Deq8v0O9lALk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
      ],
      "metadata": {
        "id": "HMIgb7o-pQZZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOBII7IPpZ8c",
        "outputId": "3a0f2168-f4f0-4846-a82e-5fca6d6e84cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "wpt = WordPunctTokenizer()\n",
        "print(wpt.tokenize(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idXwSQlHpng0",
        "outputId": "1fb867fa-c8c1-4396-c583-64932b514531"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "print(text_to_word_sequence(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I15p40zKqIXC",
        "outputId": "c2cd09fe-8826-4f55-ced3-478d746df3f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tok = TreebankWordTokenizer()\n",
        "print(tok.tokenize(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz5A69eoqrnz",
        "outputId": "e9392756-a256-4450-a07e-85ad97c5f30c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) 문장 토근화"
      ],
      "metadata": {
        "id": "O8Wsxe3grB5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg39ggEGrZNV",
        "outputId": "07e90da9-eed2-460f-844c-0d3a59494fea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sent_tokenize(text):\n",
        "    print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UVr1XR1sG6S",
        "outputId": "76af0f4e-a449-4a0b-c2b4-1d8ad3553153"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['His', 'barber', 'kept', 'his', 'word', '.']\n",
            "['But', 'keeping', 'such', 'a', 'huge', 'secret', 'to', 'himself', 'was', 'driving', 'him', 'crazy', '.']\n",
            "['Finally', ',', 'the', 'barber', 'went', 'up', 'a', 'mountain', 'and', 'almost', 'to', 'the', 'edge', 'of', 'a', 'cliff', '.']\n",
            "['He', 'dug', 'a', 'hole', 'in', 'the', 'midst', 'of', 'some', 'reeds', '.']\n",
            "['He', 'looked', 'about', ',', 'to', 'make', 'sure', 'no', 'one', 'was', 'near', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"\n",
        "print(sent_tokenize(text2))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCX4_9sCsjuC",
        "outputId": "61e78e03-8343-4f86-ace4-d1dbedc7d8ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한글문장 토큰화"
      ],
      "metadata": {
        "id": "xvaDpuOts5ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KSS(Korean Sentence Splitter)\n",
        "!pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-vCt9cmtLCh",
        "outputId": "edf5e983-ed34-4c32-cc68-f6fbc81bc505"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kss in /usr/local/lib/python3.7/dist-packages (3.3.1.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (from kss) (1.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
        "print(kss.split_sentences(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_JGwHXft5DX",
        "outputId": "bbbfb356-f468-4492-c3b5-cf5b38666470"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3) 품사(POS:Part of Speech)태깅"
      ],
      "metadata": {
        "id": "bwZNSZqeyEOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96yiVL-H1APN",
        "outputId": "88fd0e58-0bbb-4a9c-f262-0eccaaf5983e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu83HJvm14L2",
        "outputId": "bf4bb30a-a3fa-45f4-c510-874d619168cc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "pos_tag(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJirdEVA11G2",
        "outputId": "827377ac-ad2a-48fb-cfe0-0f74259d9145"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('actively', 'RB'),\n",
              " ('looking', 'VBG'),\n",
              " ('for', 'IN'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('students', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('and', 'CC'),\n",
              " ('you', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('a', 'DT'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('student', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한글(KoNLPY  : 코엔엘파이)"
      ],
      "metadata": {
        "id": "R8YmFBV62A4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Konlpy 설치\n",
        "!pip install Konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmKQRO3O3Yot",
        "outputId": "6d45b4ba-e2e8-4df2-adaf-f95fc7708eca"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from Konlpy) (4.2.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from Konlpy) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from Konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->Konlpy) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Okt(Open Korean Text)"
      ],
      "metadata": {
        "id": "6zuM8AGH3hvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 형태소 분석\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
        "okt.morphs(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27SZuBcL4RYt",
        "outputId": "6fab44da-7e5f-4f70-e79a-0d14337c46c7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "okt.morphs(text, stem=True)     # 동사 원형"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIWE9hhK4fYr",
        "outputId": "3842da00-fa0f-4160-e07d-9d76c2ca4c8c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가보다']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 품사 부착\n",
        "okt.pos(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx3rkCSF40rk",
        "outputId": "0feb6bd7-df6a-4815-9127-a59ab423443d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('열심히', 'Adverb'),\n",
              " ('코딩', 'Noun'),\n",
              " ('한', 'Josa'),\n",
              " ('당신', 'Noun'),\n",
              " (',', 'Punctuation'),\n",
              " ('연휴', 'Noun'),\n",
              " ('에는', 'Josa'),\n",
              " ('여행', 'Noun'),\n",
              " ('을', 'Josa'),\n",
              " ('가봐요', 'Verb')]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 명사 추출   <- WordCloud 할때 쓰이는 경우가 많다.\n",
        "okt.nouns(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz1Inc5n5GQG",
        "outputId": "60e7a957-950e-4ac7-c0ac-99332bf68140"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['코딩', '당신', '연휴', '여행']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 꼬꼬마"
      ],
      "metadata": {
        "id": "MOMvfI-w5Zw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "kkma.morphs(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5uUqS0x56jD",
        "outputId": "6cb70b2e-eb2c-4b1d-af69-5f71aa277656"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kkma.nouns(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9GUseRP6Etq",
        "outputId": "7d4ba281-cd82-46c7-b049-b9b56cda66e6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['코딩', '당신', '연휴', '여행']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 정제(Cleaning)과 정규화(Normalization)\n"
      ],
      "metadata": {
        "id": "GN7-J9sV6bES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\""
      ],
      "metadata": {
        "id": "lhzoi_au7CYo"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "print(shortword.sub('', text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnuzmDLr_osk",
        "outputId": "8e92a084-574a-4b98-eb39-ef21d0bc5fd3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " was wondering anyone out there could enlighten this car.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화한 후 길이가 2보다 큰 단어만 발췌\n",
        "[word for word in word_tokenize(text) if len(word) > 2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkY9m4kZ_6IX",
        "outputId": "bb750e0c-7133-418e-e553-62331e2d11f4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['was',\n",
              " 'wondering',\n",
              " 'anyone',\n",
              " 'out',\n",
              " 'there',\n",
              " 'could',\n",
              " 'enlighten',\n",
              " 'this',\n",
              " 'car']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text = ' '.join([word for word in word_tokenize(text) if len(word) > 2])\n",
        "clean_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ml7mjjrXAVxo",
        "outputId": "658ec572-00d1-474f-d8da-15865e4b7131"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'was wondering anyone out there could enlighten this car'"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. 어간 추출(Stemming) 및 표제어 추출(Lemmatization)"
      ],
      "metadata": {
        "id": "vtbvMgz8AcOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) 표제어 추출"
      ],
      "metadata": {
        "id": "A0cTsGVOBWco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IceuV6aJCCtq",
        "outputId": "b4603d68-f57a-4361-d263-7591493d074a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "VVLKYvPJCGEH"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']"
      ],
      "metadata": {
        "id": "9eAW1vkOCYNp"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "([lemma.lemmatize(word)for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCYF2tojCboD",
        "outputId": "fcde0132-bee1-4e81-a245-484d480eae1d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['policy',\n",
              " 'doing',\n",
              " 'organization',\n",
              " 'have',\n",
              " 'going',\n",
              " 'love',\n",
              " 'life',\n",
              " 'fly',\n",
              " 'dy',\n",
              " 'watched',\n",
              " 'ha',\n",
              " 'starting']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('doing', 'v'), lemmatizer.lemmatize('watched', 'v'), lemmatizer.lemmatize('has', 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhMVMSoQCx35",
        "outputId": "53197c95-3421-4cc6-e22e-9a222512e1ce"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('do', 'watch', 'have')"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma.lemmatize('flies', 'v'), lemma.lemmatize('flies', 'n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XMRItscDfcD",
        "outputId": "774bdea9-5dd7-400a-ae35-e15b73c2646b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fly', 'fly')"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) 어간 추출(Stemming)"
      ],
      "metadata": {
        "id": "LkZ7dUOEDwxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "text = \"his was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n"
      ],
      "metadata": {
        "id": "fA7ilVvqEQLN"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어간 추출 전\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riUf05SFEnOb",
        "outputId": "8b971c19-820e-4f29-d26f-5d06ff301399"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['his', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 어간 추출 후\n",
        "print([ps.stem(word) for word in word_tokenize(text)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqliahYnEpiE",
        "outputId": "ee0418fd-3655-487f-9b40-725288f001dd"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['formalize', 'allowance', 'electricical']\n",
        "print([ps.stem(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqwiX3XlEtxL",
        "outputId": "7f7b5c2e-c5fe-46b5-aebb-4117c780b923"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']"
      ],
      "metadata": {
        "id": "LNP7aWM1GDnR"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lancaster Stemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "ls = LancasterStemmer()\n",
        "print([ls.stem(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVrK-kBaFcZA",
        "outputId": "929a11cf-cf95-4fe4-dbdf-937d0989bc3b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UByPKV2MF-2d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}